#!/usr/bin/env python3
"""
ã‚¸ãƒ æ··é›‘çŠ¶æ³ ç”»åƒãƒ™ãƒ¼ã‚¹å®Œå…¨è‡ªå‹•åŒ–ã‚·ã‚¹ãƒ†ãƒ ï¼ˆç„¡æ–™OCRç‰ˆï¼‰
- iPhoneã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆ â†’ ç„¡æ–™OCR â†’ CSVçµ±åˆ
- æ—¢å­˜ã®æ­£è¦è¡¨ç¾ãƒ­ã‚¸ãƒƒã‚¯ã‚’æœ€å¤§æ´»ç”¨
- å®Œå…¨ç„¡æ–™å®Ÿè£…ï¼ˆEasyOCR + Tesseractï¼‰
"""

import csv
import json
import re
import os
import shutil
import datetime as dt
from pathlib import Path
import logging
from collections import defaultdict

# ç„¡æ–™OCRãƒ©ã‚¤ãƒ–ãƒ©ãƒª
try:
    import easyocr
    EASYOCR_AVAILABLE = True
except ImportError:
    EASYOCR_AVAILABLE = False
    logging.warning("EasyOCR not available. Install with: pip install easyocr")

try:
    import pytesseract
    from PIL import Image
    TESSERACT_AVAILABLE = True
except ImportError:
    TESSERACT_AVAILABLE = False
    logging.warning("Tesseract not available. Install with: pip install pytesseract pillow")


class GymImageOCRPipeline:
    def __init__(self):
        self.project_dir = Path("/Users/i_kawano/Documents/training_waitnum_analysis")
        self.csv_file = self.project_dir / "data" / "fit_place24_data.csv"
        self.backup_dir = self.project_dir / "backups"
        self.log_file = self.project_dir / "logs" / "weekly_ocr.log"
        
        # ç”»åƒå‡¦ç†é–¢é€£ãƒ‘ã‚¹
        self.icloud_images = Path.home() / "Library/Mobile Documents/iCloud~is~workflow~my~workflows/Documents/FIT_PLACE24"
        self.archive_base = self.project_dir / "archive" / "screens"
        self.processed_dir = self.archive_base / "processed"
        self.failed_dir = self.archive_base / "failed"
        
        # ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ãƒãƒƒãƒ”ãƒ³ã‚°ï¼ˆæ—¢å­˜ãƒ­ã‚¸ãƒƒã‚¯æµç”¨ï¼‰
        self.status_map = {
            "ç©ºã„ã¦ã„ã¾ã™": "low",
            "ã‚„ã‚„ç©ºã„ã¦ã„ã¾ã™": "mid-low", 
            "æ™®é€š": "mid",
            "ã‚„ã‚„æ··é›‘": "mid-high",
            "ã‚„ã‚„æ··ã‚“ã§ã„ã¾ã™": "mid-high",
            "æ··é›‘": "high",
            "æ··ã‚“ã§ã„ã¾ã™": "high",
            "ã‹ãªã‚Šæ··é›‘": "very-high",
        }
        
        self._setup_directories()
        self._setup_logging()
        
        # OCRã‚¨ãƒ³ã‚¸ãƒ³åˆæœŸåŒ–ï¼ˆãƒ­ã‚°è¨­å®šå¾Œã«å®Ÿè¡Œï¼‰
        self.easyocr_reader = None
        if EASYOCR_AVAILABLE:
            try:
                self.easyocr_reader = easyocr.Reader(['ja', 'en'], gpu=False)
                self.logger.info("EasyOCRåˆæœŸåŒ–å®Œäº†")
            except Exception as e:
                self.logger.warning(f"EasyOCRåˆæœŸåŒ–å¤±æ•—: {e}")

    def _setup_directories(self):
        """å¿…è¦ãªãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ"""
        self.backup_dir.mkdir(parents=True, exist_ok=True)
        self.csv_file.parent.mkdir(parents=True, exist_ok=True)
        self.log_file.parent.mkdir(parents=True, exist_ok=True)
        self.processed_dir.mkdir(parents=True, exist_ok=True)
        self.failed_dir.mkdir(parents=True, exist_ok=True)

    def _setup_logging(self):
        """ãƒ­ã‚°è¨­å®š"""
        logging.basicConfig(
            level=logging.INFO,
            format="%(asctime)s - %(levelname)s - %(message)s",
            handlers=[
                logging.FileHandler(self.log_file, encoding="utf-8"),
                logging.StreamHandler(),
            ],
        )
        self.logger = logging.getLogger(__name__)

    def find_new_images(self):
        """iCloudã‹ã‚‰æ–°ã—ã„ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢"""
        if not self.icloud_images.exists():
            self.logger.error(f"iCloudãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒå­˜åœ¨ã—ã¾ã›ã‚“: {self.icloud_images}")
            return []
        
        # PNGç”»åƒã‚’æ¤œç´¢ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«åãƒ‘ã‚¿ãƒ¼ãƒ³: FP24_20250815_222321.png or 2025:08:15, 22:23.pngï¼‰
        image_files = []
        for pattern in ["*.png", "*.PNG", "*.jpg", "*.JPEG"]:
            for img_path in self.icloud_images.glob(pattern):
                if img_path.is_file():
                    image_files.append(img_path)
        
        self.logger.info(f"iCloudã‹ã‚‰{len(image_files)}å€‹ã®ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç™ºè¦‹")
        return sorted(image_files, key=lambda x: x.stat().st_mtime)

    def extract_text_from_image(self, image_path):
        """ç”»åƒã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡ºï¼ˆEasyOCR â†’ Tesseract ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰"""
        extracted_text = ""
        
        # Primary: EasyOCR
        if self.easyocr_reader:
            try:
                results = self.easyocr_reader.readtext(str(image_path))
                text_parts = [result[1] for result in results if result[2] > 0.3]  # ä¿¡é ¼åº¦30%ä»¥ä¸Š
                extracted_text = " ".join(text_parts)
                self.logger.info(f"EasyOCRæŠ½å‡ºæˆåŠŸ: {len(text_parts)}å€‹ã®ãƒ†ã‚­ã‚¹ãƒˆè¦ç´ ")
            except Exception as e:
                self.logger.warning(f"EasyOCRå¤±æ•—: {e}")
        
        # Fallback: Tesseract OCR
        if not extracted_text and TESSERACT_AVAILABLE:
            try:
                image = Image.open(image_path)
                extracted_text = pytesseract.image_to_string(image, lang='jpn+eng')
                self.logger.info("Tesseract OCRæŠ½å‡ºæˆåŠŸ")
            except Exception as e:
                self.logger.warning(f"Tesseract OCRå¤±æ•—: {e}")
        
        if not extracted_text:
            self.logger.error(f"OCRæŠ½å‡ºå¤±æ•—: {image_path}")
        
        return extracted_text.strip()

    def parse_filename_timestamp(self, image_path):
        """ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰æ—¥æ™‚æƒ…å ±ã‚’æŠ½å‡º"""
        filename = image_path.name
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³1: 2025:08:15, 22:23.png (iOS Shortcutå½¢å¼)
        pattern1 = r"(\d{4}):(\d{2}):(\d{2}),\s*(\d{2}):(\d{2})"
        match1 = re.search(pattern1, filename)
        if match1:
            year, month, day, hour, minute = match1.groups()
            try:
                dt_obj = dt.datetime(int(year), int(month), int(day), int(hour), int(minute))
                return dt_obj
            except ValueError as e:
                self.logger.warning(f"æ—¥æ™‚è§£æã‚¨ãƒ©ãƒ¼ (ãƒ‘ã‚¿ãƒ¼ãƒ³1): {e}")
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³2: FP24_20250815_222321.png
        pattern2 = r"FP24_(\d{8})_(\d{6})"
        match2 = re.search(pattern2, filename)
        if match2:
            date_part, time_part = match2.groups()
            try:
                dt_obj = dt.datetime.strptime(f"{date_part}_{time_part}", "%Y%m%d_%H%M%S")
                return dt_obj
            except ValueError as e:
                self.logger.warning(f"æ—¥æ™‚è§£æã‚¨ãƒ©ãƒ¼ (ãƒ‘ã‚¿ãƒ¼ãƒ³2): {e}")
        
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ãƒ•ã‚¡ã‚¤ãƒ«ã®å¤‰æ›´æ—¥æ™‚
        file_mtime = dt.datetime.fromtimestamp(image_path.stat().st_mtime)
        self.logger.info(f"ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰æ—¥æ™‚æŠ½å‡ºå¤±æ•—ã€ãƒ•ã‚¡ã‚¤ãƒ«æ›´æ–°æ—¥æ™‚ã‚’ä½¿ç”¨: {file_mtime}")
        return file_mtime

    def parse_gym_data(self, text, timestamp):
        """æ—¢å­˜ã®æ­£è¦è¡¨ç¾ãƒ­ã‚¸ãƒƒã‚¯ã§ã‚¸ãƒ ãƒ‡ãƒ¼ã‚¿ã‚’è§£æ"""
        # äººæ•°æŠ½å‡ºï¼ˆæ—¢å­˜ãƒ­ã‚¸ãƒƒã‚¯æµç”¨ï¼‰
        people_patterns = [
            r"(\d{1,3})\s*äºº",
            r"æ··é›‘çŠ¶æ³\s*(\d{1,3})",
            r"ç¾åœ¨\s*(\d{1,3})\s*äºº",
        ]
        
        people_count = None
        for pattern in people_patterns:
            match = re.search(pattern, text)
            if match:
                people_count = int(match.group(1))
                break
        
        # ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹æŠ½å‡ºï¼ˆå®Œå…¨ä¸€è‡´ã§ã®æ­£ç¢ºãªæŠ½å‡ºï¼‰
        status_text = ""
        # å®Œå…¨ãªãƒ•ãƒ¬ãƒ¼ã‚ºã®ã¿ã‚’ãƒãƒƒãƒãƒ³ã‚°ï¼ˆå„ªå…ˆé †ä½é †ï¼‰
        exact_status_patterns = [
            "ã‚„ã‚„ç©ºã„ã¦ã„ã¾ã™",
            "å°‘ã—æ··ã‚“ã§ã„ã¾ã™", 
            "ã‹ãªã‚Šæ··ã‚“ã§ã„ã¾ã™",
            "ã‚„ã‚„æ··ã‚“ã§ã„ã¾ã™",
            "ç©ºã„ã¦ã„ã¾ã™",
            "æ··ã‚“ã§ã„ã¾ã™",
            "ã‹ãªã‚Šæ··é›‘",
            "æ™®é€š"
        ]
        
        # å®Œå…¨ä¸€è‡´ã§ã®ã¿æŠ½å‡º
        for pattern in exact_status_patterns:
            if pattern in text:
                status_text = pattern
                break
        
        # ãƒ‡ãƒ¼ã‚¿æ§‹é€ åŒ–
        if people_count is not None:
            # æ—¢å­˜ã®_generate_status_infoãƒ­ã‚¸ãƒƒã‚¯ã‚’é©ç”¨
            status_info = self._generate_status_info(people_count, status_text)
            
            converted_data = {
                "datetime": timestamp.strftime("%Y-%m-%d %H:%M:%S"),
                "date": timestamp.strftime("%Y-%m-%d"),
                "time": timestamp.strftime("%H:%M"),
                "hour": timestamp.hour,
                "weekday": timestamp.strftime("%A"),
                "count": people_count,
                "status_label": status_info["label"],
                "status_code": status_info["code"],
                "status_min": status_info["min"],
                "status_max": status_info["max"],
                "raw_text": text[:200],  # æœ€åˆã®200æ–‡å­—ã®ã¿ä¿å­˜
            }
            
            return converted_data
        
        self.logger.warning(f"äººæ•°æƒ…å ±ã®æŠ½å‡ºã«å¤±æ•—: {text[:100]}")
        return None

    def _generate_status_info(self, people_count, status_text):
        """æ­£ã—ã„5æ®µéšã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹åˆ¤å®š: äººæ•°ã¨ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰è©³ç´°ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹æƒ…å ±ã‚’ç”Ÿæˆ"""
        # ãƒ†ã‚­ã‚¹ãƒˆãƒ™ãƒ¼ã‚¹ã®åˆ¤å®šï¼ˆé †åºé‡è¦ï¼šã‚ˆã‚Šå…·ä½“çš„ãªãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å…ˆã«åˆ¤å®šï¼‰
        if "ã‚„ã‚„ç©ºã„ã¦ã„ã¾ã™" in status_text:
            return {"code": 4, "label": "ã‚„ã‚„ç©ºã„ã¦ã„ã¾ã™ï¼ˆ~20äººï¼‰", "min": 11, "max": 20}
        elif "ã‚„ã‚„æ··ã‚“ã§ã„ã¾ã™" in status_text or "å°‘ã—æ··ã‚“ã§ã„ã¾ã™" in status_text:
            return {"code": 3, "label": "å°‘ã—æ··ã‚“ã§ã„ã¾ã™ï¼ˆ~30äººï¼‰", "min": 21, "max": 30}
        elif "ã‹ãªã‚Šæ··ã‚“ã§ã„ã¾ã™" in status_text or "ã‹ãªã‚Šæ··é›‘" in status_text:
            return {"code": 1, "label": "ã‹ãªã‚Šæ··ã‚“ã§ã„ã¾ã™ï¼ˆ~50äººï¼‰", "min": 41, "max": 50}
        elif "ç©ºã„ã¦ã„ã¾ã™" in status_text:
            return {"code": 5, "label": "ç©ºã„ã¦ã„ã¾ã™ï¼ˆ~10äººï¼‰", "min": 0, "max": 10}
        elif "æ··ã‚“ã§ã„ã¾ã™" in status_text or "æ··é›‘" in status_text:
            return {"code": 2, "label": "æ··ã‚“ã§ã„ã¾ã™ï¼ˆ~40äººï¼‰", "min": 31, "max": 40}
        else:
            # äººæ•°ãƒ™ãƒ¼ã‚¹ã®åˆ¤å®š
            if people_count <= 10:
                return {"code": 5, "label": "ç©ºã„ã¦ã„ã¾ã™ï¼ˆ~10äººï¼‰", "min": 0, "max": 10}
            elif people_count <= 20:
                return {"code": 4, "label": "ã‚„ã‚„ç©ºã„ã¦ã„ã¾ã™ï¼ˆ~20äººï¼‰", "min": 11, "max": 20}
            elif people_count <= 30:
                return {"code": 3, "label": "å°‘ã—æ··ã‚“ã§ã„ã¾ã™ï¼ˆ~30äººï¼‰", "min": 21, "max": 30}
            elif people_count <= 40:
                return {"code": 2, "label": "æ··ã‚“ã§ã„ã¾ã™ï¼ˆ~40äººï¼‰", "min": 31, "max": 40}
            else:
                return {"code": 1, "label": "ã‹ãªã‚Šæ··ã‚“ã§ã„ã¾ã™ï¼ˆ~50äººï¼‰", "min": 41, "max": 50}

    def read_existing_csv_data(self):
        """æ—¢å­˜ã®CSVãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ï¼ˆæ—¢å­˜ãƒ­ã‚¸ãƒƒã‚¯æµç”¨ï¼‰"""
        existing_data = []
        existing_keys = set()
        
        if self.csv_file.exists():
            try:
                with self.csv_file.open(newline="", encoding="utf-8") as f:
                    reader = csv.DictReader(f)
                    for row in reader:
                        existing_data.append(row)
                        # é‡è¤‡ãƒã‚§ãƒƒã‚¯ç”¨ã‚­ãƒ¼ï¼ˆæ—¥æ™‚+å ´æ‰€+äººæ•°ï¼‰
                        key = (row.get("datetime", ""), "çŸ¢å‘", str(row.get("count", "")))
                        existing_keys.add(key)
            except Exception as e:
                self.logger.error(f"æ—¢å­˜CSVèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        
        return existing_data, existing_keys

    def dedupe_data(self, all_data):
        """é‡è¤‡ãƒ‡ãƒ¼ã‚¿ã‚’é™¤å»ï¼ˆæ—¢å­˜ãƒ­ã‚¸ãƒƒã‚¯æµç”¨ï¼‰"""
        seen = set()
        unique_data = []
        
        for row in sorted(all_data, key=lambda x: x.get("datetime", "")):
            # é‡è¤‡ãƒã‚§ãƒƒã‚¯ã‚­ãƒ¼
            key = (row.get("datetime", ""), "çŸ¢å‘", str(row.get("count", "")))
            
            if key in seen:
                self.logger.debug(f"é‡è¤‡ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¹ã‚­ãƒƒãƒ—: {key}")
                continue
            
            seen.add(key)
            unique_data.append(row)
        
        removed = len(all_data) - len(unique_data)
        if removed > 0:
            self.logger.info(f"é‡è¤‡ãƒ‡ãƒ¼ã‚¿{removed}ä»¶ã‚’é™¤å»")
        
        return unique_data

    def write_csv(self, data):
        """CSVãƒ•ã‚¡ã‚¤ãƒ«ã«æ›¸ãè¾¼ã¿ï¼ˆæ—¢å­˜ãƒ­ã‚¸ãƒƒã‚¯æµç”¨ï¼‰"""
        fieldnames = [
            "datetime", "date", "time", "hour", "weekday",
            "count", "status_label", "status_code", "status_min", "status_max", "raw_text"
        ]
        
        # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã«æ›¸ãè¾¼ã¿å¾Œã€ã‚¢ãƒˆãƒŸãƒƒã‚¯ç§»å‹•
        tmp_file = self.csv_file.with_suffix(".tmp.csv")
        
        try:
            with tmp_file.open("w", encoding="utf-8", newline="") as f:
                writer = csv.DictWriter(f, fieldnames=fieldnames)
                writer.writeheader()
                
                for row in data:
                    # ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰åã«åˆã‚ã›ã¦ãƒ‡ãƒ¼ã‚¿ã‚’æ•´ç†
                    clean_row = {field: row.get(field, "") for field in fieldnames}
                    writer.writerow(clean_row)
            
            # ã‚¢ãƒˆãƒŸãƒƒã‚¯ç§»å‹•
            shutil.move(str(tmp_file), str(self.csv_file))
            self.logger.info(f"CSVãƒ•ã‚¡ã‚¤ãƒ«æ›´æ–°å®Œäº†: {len(data)}ä»¶")
            return True
            
        except Exception as e:
            if tmp_file.exists():
                tmp_file.unlink()
            self.logger.error(f"CSVæ›¸ãè¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            return False

    def archive_image(self, image_path, success=True):
        """å‡¦ç†æ¸ˆã¿ç”»åƒã‚’ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–"""
        timestamp = dt.datetime.now().strftime("%Y%m%d_%H%M%S")
        archive_dir = self.processed_dir if success else self.failed_dir
        
        new_name = f"{timestamp}_{image_path.name}"
        archive_path = archive_dir / new_name
        
        try:
            shutil.move(str(image_path), str(archive_path))
            status = "processed" if success else "failed"
            self.logger.info(f"ç”»åƒã‚’{status}ã«ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–: {new_name}")
            return True
        except Exception as e:
            self.logger.error(f"ç”»åƒã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ã‚¨ãƒ©ãƒ¼: {e}")
            return False

    def analyze_data(self):
        """ãƒ‡ãƒ¼ã‚¿åˆ†æã‚’å®Ÿè¡Œï¼ˆæ—¢å­˜ãƒ­ã‚¸ãƒƒã‚¯æµç”¨ï¼‰"""
        try:
            existing_data, _ = self.read_existing_csv_data()
            
            if not existing_data:
                self.logger.warning("åˆ†æå¯¾è±¡ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
                return
            
            # æ™‚é–“å¸¯åˆ¥åˆ†æ
            hourly_analysis = defaultdict(list)
            
            for row in existing_data:
                if row.get("hour") and row.get("count"):
                    try:
                        hour = int(row["hour"])
                        count = int(row["count"])
                        hourly_analysis[hour].append(count)
                    except ValueError:
                        continue
            
            self.logger.info(f"ğŸ“Š ãƒ‡ãƒ¼ã‚¿åˆ†æçµæœï¼ˆç·ãƒ‡ãƒ¼ã‚¿æ•°: {len(existing_data)}ä»¶ï¼‰")
            
            # æœ€é©æ™‚é–“å¸¯
            best_times = []
            for hour in sorted(hourly_analysis.keys()):
                counts = hourly_analysis[hour]
                avg_count = sum(counts) / len(counts)
                if avg_count <= 15:
                    best_times.append((hour, avg_count))
            
            if best_times:
                best_times.sort(key=lambda x: x[1])
                self.logger.info("ğŸ¯ æœ€é©åˆ©ç”¨æ™‚é–“å¸¯ï¼ˆç©ºã„ã¦ã„ã‚‹æ™‚é–“å¸¯ï¼‰:")
                for hour, avg in best_times:
                    self.logger.info(f"  {hour:2d}:00 - å¹³å‡ {avg:.1f}äºº â­ï¸")
            
        except Exception as e:
            self.logger.error(f"åˆ†æã‚¨ãƒ©ãƒ¼: {e}")

    def run_weekly_ocr_pipeline(self):
        """é€±æ¬¡ç”»åƒOCRå‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ï¼ˆãƒ¡ã‚¤ãƒ³å‡¦ç†ï¼‰"""
        self.logger.info("ğŸš€ é€±æ¬¡ç”»åƒOCRå‡¦ç†ã‚’é–‹å§‹ã—ã¾ã™...")
        
        try:
            # 1. æ–°ã—ã„ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢
            self.logger.info("ğŸ“‚ iCloudã‹ã‚‰æ–°ã—ã„ç”»åƒã‚’æ¤œç´¢ä¸­...")
            image_files = self.find_new_images()
            
            if not image_files:
                self.logger.info("ğŸ“‹ æ–°ã—ã„ç”»åƒã¯ã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
                return True
            
            # 2. ç”»åƒã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º
            self.logger.info(f"ğŸ” {len(image_files)}å€‹ã®ç”»åƒã‚’å‡¦ç†ä¸­...")
            new_data = []
            processed_count = 0
            failed_count = 0
            
            for image_path in image_files:
                try:
                    # OCRã§ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º
                    extracted_text = self.extract_text_from_image(image_path)
                    if not extracted_text:
                        self.archive_image(image_path, success=False)
                        failed_count += 1
                        continue
                    
                    # ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰æ—¥æ™‚æŠ½å‡º
                    timestamp = self.parse_filename_timestamp(image_path)
                    
                    # ãƒ‡ãƒ¼ã‚¿è§£æ
                    parsed_data = self.parse_gym_data(extracted_text, timestamp)
                    if parsed_data:
                        new_data.append(parsed_data)
                        self.archive_image(image_path, success=True)
                        processed_count += 1
                        self.logger.info(f"âœ… å‡¦ç†æˆåŠŸ: {image_path.name} -> {parsed_data['count']}äºº")
                    else:
                        self.archive_image(image_path, success=False)
                        failed_count += 1
                        
                except Exception as e:
                    self.logger.error(f"ç”»åƒå‡¦ç†ã‚¨ãƒ©ãƒ¼ {image_path.name}: {e}")
                    self.archive_image(image_path, success=False)
                    failed_count += 1
            
            self.logger.info(f"ğŸ“Š ç”»åƒå‡¦ç†å®Œäº†: æˆåŠŸ{processed_count}ä»¶, å¤±æ•—{failed_count}ä»¶")
            
            if not new_data:
                self.logger.warning("âš ï¸ å‡¦ç†å¯èƒ½ãªãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
                return True
            
            # 3. æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã¨çµ±åˆ
            self.logger.info("ğŸ”— æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã¨çµ±åˆä¸­...")
            existing_data, _ = self.read_existing_csv_data()
            all_data = existing_data + new_data
            
            # 4. é‡è¤‡é™¤å»
            unique_data = self.dedupe_data(all_data)
            
            # 5. CSVãƒ•ã‚¡ã‚¤ãƒ«æ›´æ–°
            self.logger.info("ğŸ’¾ CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ›´æ–°ä¸­...")
            if not self.write_csv(unique_data):
                self.logger.error("âŒ CSVæ›´æ–°ã«å¤±æ•—")
                return False
            
            new_count = len(new_data)
            total_count = len(unique_data)
            
            self.logger.info(f"âœ… {new_count}ä»¶ã®æ–°ãƒ‡ãƒ¼ã‚¿ã‚’è¿½åŠ ")
            self.logger.info(f"ğŸ“ ç·ãƒ‡ãƒ¼ã‚¿æ•°: {total_count}ä»¶")
            
            # 6. ãƒ‡ãƒ¼ã‚¿åˆ†æ
            self.logger.info("ğŸ“Š ãƒ‡ãƒ¼ã‚¿åˆ†æã‚’å®Ÿè¡Œä¸­...")
            self.analyze_data()
            
            self.logger.info("ğŸ‰ é€±æ¬¡ç”»åƒOCRå‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸï¼")
            return True
            
        except Exception as e:
            self.logger.error(f"é€±æ¬¡OCRå‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
            return False

    def diagnose_system(self):
        """ã‚·ã‚¹ãƒ†ãƒ è¨ºæ–­"""
        self.logger.info("ğŸ” ã‚·ã‚¹ãƒ†ãƒ è¨ºæ–­ã‚’é–‹å§‹...")
        
        # iCloudãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ç¢ºèª
        if not self.icloud_images.exists():
            self.logger.error(f"âŒ iCloudãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒå­˜åœ¨ã—ã¾ã›ã‚“: {self.icloud_images}")
            return False
        
        # ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ã®ç¢ºèª
        image_files = self.find_new_images()
        self.logger.info(f"ğŸ“‚ iCloudç”»åƒãƒ•ã‚¡ã‚¤ãƒ«: {len(image_files)}å€‹")
        
        # OCRã‚¨ãƒ³ã‚¸ãƒ³ã®ç¢ºèª
        if self.easyocr_reader:
            self.logger.info("âœ… EasyOCR: åˆ©ç”¨å¯èƒ½")
        else:
            self.logger.warning("âš ï¸ EasyOCR: åˆ©ç”¨ä¸å¯")
        
        if TESSERACT_AVAILABLE:
            self.logger.info("âœ… Tesseract OCR: åˆ©ç”¨å¯èƒ½")
        else:
            self.logger.warning("âš ï¸ Tesseract OCR: åˆ©ç”¨ä¸å¯")
        
        # æ—¢å­˜CSVã®ç¢ºèª
        if self.csv_file.exists():
            existing_data, _ = self.read_existing_csv_data()
            self.logger.info(f"ğŸ’¾ æ—¢å­˜CSV: {len(existing_data)}ä»¶ã®ãƒ‡ãƒ¼ã‚¿")
        else:
            self.logger.info("ğŸ’¾ æ—¢å­˜CSV: ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ã¾ã›ã‚“")
        
        # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ¨©é™ã®ç¢ºèª
        for path in [self.icloud_images, self.processed_dir, self.failed_dir, self.csv_file.parent]:
            if path.exists() and os.access(path, os.R_OK | os.W_OK):
                self.logger.info(f"âœ… {path.name}: èª­ã¿æ›¸ãæ¨©é™OK")
            else:
                self.logger.warning(f"âš ï¸ {path.name}: æ¨©é™ä¸è¶³ã¾ãŸã¯å­˜åœ¨ã—ãªã„")
        
        self.logger.info("ğŸ” ã‚·ã‚¹ãƒ†ãƒ è¨ºæ–­å®Œäº†")
        return True


def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    pipeline = GymImageOCRPipeline()
    
    import sys
    
    if len(sys.argv) > 1:
        command = sys.argv[1]
        if command == "--weekly":
            pipeline.run_weekly_ocr_pipeline()
        elif command == "diagnose":
            pipeline.diagnose_system()
        elif command == "analyze":
            pipeline.analyze_data()
        else:
            print(f"âŒ ä¸æ˜ãªã‚³ãƒãƒ³ãƒ‰: {command}")
            print("åˆ©ç”¨å¯èƒ½ãªã‚³ãƒãƒ³ãƒ‰: --weekly, diagnose, analyze")
    else:
        # ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ¢ãƒ¼ãƒ‰
        print("ğŸ¤– ã‚¸ãƒ æ··é›‘çŠ¶æ³ ç”»åƒOCRè‡ªå‹•åŒ–ã‚·ã‚¹ãƒ†ãƒ ï¼ˆç„¡æ–™ç‰ˆï¼‰")
        print("=" * 60)
        print("1. ğŸš€ é€±æ¬¡ç”»åƒOCRå®Ÿè¡Œ")
        print("2. ğŸ” ã‚·ã‚¹ãƒ†ãƒ è¨ºæ–­")
        print("3. ğŸ“Š ãƒ‡ãƒ¼ã‚¿åˆ†æã®ã¿")
        
        choice = input("\\né¸æŠã—ã¦ãã ã•ã„ (1-3): ")
        
        if choice == "1":
            pipeline.run_weekly_ocr_pipeline()
        elif choice == "2":
            pipeline.diagnose_system()
        elif choice == "3":
            pipeline.analyze_data()
        else:
            print("âŒ ç„¡åŠ¹ãªé¸æŠã§ã™")


if __name__ == "__main__":
    main()